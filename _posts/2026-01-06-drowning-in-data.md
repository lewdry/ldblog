---
layout: post
title: "Drowning in Data: How Information Overload is Messing with Our Decisions"
date: 2026-01-06
description: "A review of recent research on information overload, decision paralysis, cognitive limits and how social media and misinformation affect our ability to make good decisions."
tags: [information-overload, decision-making, cognitive-bias, choice-overload, social-media, misinformation, bounded-rationality, research]
categories: [Information overload, decision making]
author: ["Lewis Dryburgh"]
permalink: /2026-01-06-drowning-in-data
---

This is the second in a series of... well, at least two desktop literature reviews I've been writing to flex my withered grey cells.

While I've had some time and space to trawl through recent open-access papers over the Australian summer holidays, I am expecting the arrival of a third child in February. So I make no commitments to future output, but reader, I am enjoying hard-staring at interesting topics. 

After last month's review of [Information Seeking and Gen AI](https://lewisdryburgh.com/2025-12-29-just-checking), which was peppered with some scepticism, I thought I'd try and (mostly) escape the AI zeitgeist and look into how we're being impacted by **information overload**.

<img src="{{ '/assets/images/tatsuo2.jpg' | relative_url }}" alt="Tatsuo Miyajima, Connecting with Everything, 2017" style="width: 85%; display: block; margin: 0 auto;"> 
<p style="text-align: center;"><em>Tatsuo Miyajima, Connecting with Everything, 2017</em></p>

This is a term which has rattled around my head for almost a decade. Both in my personal life, and at work, I have spent a lot of time thinking about the increase in information I am bombarded with on a daily basis. Back in 2017, when I was working as a trainer, I actually taught a course called *'Information Overload'*, meant to help attendees find and verify trustworthy information.

In hindsight, 2017 seems like the good old days compared to where we are now. (Cue The Office quote: "I wish there was a way to know you were in the good old days..."). If you can cast your mind back to that innocent era:

- We didn't know about Cambridge Analytica's voter manipulation (although we'd been hit by it), they were finally exposed in March 2018.
- We had no idea the possible depths and volume of misinformation which would emerge through the covid pandemic.
- TikTok was barely up and running, with *only* 100 million users at the end of 2016 (vs ~1.5 billion today)
- The once interesting, influential and beloved (to some) Twitter was independent and years away from the 'free speech' takeover and transition to X.

Phew. And that's not to mention the slop we're being served by AI, because I'm trying not to mention it this month.

So, waffle aside. I have tried to pull together what I think is a useful review on *the cognitive costs of too much information in the mid-2020s* and what we can do about it. As with my previous review, I have sourced predominantly from recent, open-access research, along with a few classics for grounding.

This review argues that contemporary information overload is not simply a problem of too much information, but a systemic mismatch between human cognitive limits and digitally engineered information environments. When overload interacts with social influence, algorithmic amplification and emotional salience, it degrades decision quality, increases reliance on cognitive shortcuts and accelerates the spread of misinformation. While individual coping strategies help at the margins, the evidence increasingly suggests that meaningful improvement requires structural and regulatory intervention alongside improvements in information literacy.

### Jump To

- [The Information Paradox](#the-information-paradox)
- [The Limits of the Human Mind](#the-limits-of-the-human-mind)
- [Decision Paralysis: Too Many Choices, Too Little Action](#decision-paralysis-too-many-choices-too-little-action)
- [The Social Media Factor](#the-social-media-factor)
- [The Misinformation Crisis](#the-misinformation-crisis)
- [What Can We Do About It?](#what-can-we-do-about-it)
  - [Individual Strategies](#individual-strategies)
  - [Structural and Design Interventions](#structural-and-design-interventions)
  - [The Role of Regulation](#the-role-of-regulation)
- [Looking Ahead](#looking-ahead)
- [References](#references)

### Scope and approach

This is a desktop literature review drawing primarily on recent (2021–2025), open-access research across psychology, information systems, media studies and public health. Sources were identified through targeted searches of Google Scholar and publisher databases using terms including information overload, choice overload, decision paralysis, misinformation sharing and social media cognition. Foundational “classic” works are included where they remain theoretically influential (e.g. Simon, Miller, Kahneman & Tversky). The review prioritises empirical studies, systematic reviews and meta-analyses, but is not exhaustive.

### The Information Paradox

We've never had more information at our fingertips. A few taps on a smartphone can summon answers to virtually any question, from restaurant reviews to medical advice to breaking news from around the globe. Yet paradoxically, all this access to information hasn't made us better decision-makers. If anything, the sheer volume of information we consume daily might be making our decisions worse.

Researchers have been studying **information overload**, for decades. The concept describes what happens when the amount of information available exceeds our capacity to process it effectively (Eppler & Mengis, 2004). It's like trying to drink from a fire hose: there's plenty of water, but you're more likely to get knocked over than hydrated. As the digital landscape has exploded with social media, 24/7 news cycles and AI-generated content, understanding how information overload affects our decision-making and what we can do about it, has become more important than ever.

### The Limits of the Human Mind

To understand why information overload is such a problem, we need to first appreciate the limitations of human cognition. Back in 1956, psychologist George Miller published his famous paper on "The Magical Number Seven, Plus or Minus Two," demonstrating that human working memory can only hold about seven pieces of information at a time (Miller, 1956). This is a feature of how our brains evolved, but it creates a fundamental bottleneck when we're faced with the endless streams of data flowing through modern life.

This bottleneck highlights a crucial distinction made by Clay Shirky (2008) nearly two decades ago: 'It’s not information overload. It’s filter failure.' In the past, physical and economic barriers acted as filters. Publishers, editors,and limited broadcast hours curated what reached us. Today, those external filters have collapsed, leaving our biological filters to do all the heavy lifting against an infinite stream. We are trying to use a biological sieve to hold back a digital tsunami.

Herbert Simon, who would later win a Nobel Prize for his work on decision-making, recognised this problem early. He proposed the concept of **bounded rationality**: the idea that humans don't make perfectly rational decisions because we simply don't have the cognitive resources to weigh every option (Simon, 1955). Instead, we "satisfice": we look for options that are *good enough* rather than optimal. This works fine when we're choosing what to have for lunch, but it can lead to serious problems when the stakes are higher.

When we're overloaded with information, our decision-making shortcuts can backfire spectacularly. Kahneman and Tversky's groundbreaking research on heuristics and biases showed that humans rely on mental shortcuts, rules of thumb, to make judgments, especially under uncertainty (Tversky & Kahneman, 1974). These heuristics are usually helpful, but they can lead us astray. The **availability heuristic**, for example, causes us to overestimate the likelihood of events that come easily to mind. So if you've just read five news articles about plane crashes, you might overestimate the danger of flying, even though the statistics say otherwise. Information overload feeds these biases by constantly flooding our minds with memorable, emotionally charged content.

### Decision Paralysis: Too Many Choices, Too Little Action

One of the most studied consequences of information overload is **choice overload**: what happens when we're faced with too many options. Intuitively, more choice should be better. But research tells a different story. When people are presented with an overwhelming number of alternatives, they often experience decision paralysis: the inability to make any choice at all (Boby, 2024; Misuraca et al., 2024).

It’s worth noting that much of the choice overload literature is still dominated by controlled experiments and platform-specific case studies. Effect sizes vary substantially and not all studies find paralysis effects, particularly when users have strong prior preferences or domain expertise. This suggests that overload is not universal, but contingent: it emerges most strongly when high option volume is paired with low preference clarity and poor choice architecture.

This phenomenon plays out across many domains. In online food ordering, for example, researchers found that an excessive number of menu options significantly increased decision paralysis, particularly among younger users who are already heavy consumers of digital content (Boby, 2024). The same pattern appears in everything from retirement savings (where too many investment options lead people to save less) to dating apps, where endless swiping can lead to paradoxically fewer meaningful connections (Thomas et al., 2025).

The moderators of choice overload are complex. Research suggests it's not just about the raw number of options, it's also about how complex those options are, how certain people are about their preferences and how the choices are presented (Misuraca et al., 2024). Clear categorisation and filtering tools can help, which is why good design matters so much in digital environments.

### The Social Media Factor

If information overload was a problem before social media, it's become a crisis in the age of endless scrolling. The platforms we use daily are specifically designed to maximize engagement, which often means maximizing the amount of stimulating content we consume. And the effects on our cognition and mental health are becoming increasingly clear.

A recent systematic review and meta-analysis of short-form video use (think TikTok, Instagram Reels) found that increased use is associated with poorer cognition, particularly attention and inhibitory control, as well as higher levels of stress and anxiety (Nguyen et al., 2025). This matters for decision-making because attentional control is exactly what we need to sift through information carefully and resist impulsive choices.

Social media also creates powerful dynamics of **social influence** that can distort our judgment. A large-scale experiment on a social news platform found that positive ratings created significant herding effects, when people saw that others had upvoted a comment, they were 32% more likely to upvote it themselves, regardless of the comment's actual quality (Muchnik et al., 2013). This positive herding accumulated over time, inflating final ratings by an average of 25%. Interestingly, negative herding was naturally corrected by users, but positive bias wasn't. This asymmetry helps explain why misinformation that gets early positive engagement can spread so rapidly.

A recurring limitation across this literature is its reliance on correlational designs and self-reported use. While associations between short-form video consumption and cognitive outcomes are robust, causal pathways remain contested, particularly given the likelihood of reciprocal effects (e.g. attentional difficulties driving platform use rather than resulting from it). Longitudinal and quasi-experimental work remains comparatively scarce.

### The Misinformation Crisis

The combination of information overload, cognitive biases and social influence creates fertile ground for misinformation. When people feel overwhelmed by information, particularly during crises like the covid pandemic, they're more likely to share content without verifying it (Huang et al., 2022). This isn't because they're careless or stupid; it's a natural coping mechanism. Sharing information creates a sense of control and connection during uncertain times. But it also means that false or misleading content can spread rapidly.

Research has identified several key drivers of unverified information sharing. **Perceived severity** plays a major role: when people believe a threat is serious, they're more motivated to share warnings, even if those warnings haven't been verified (Zhang et al., 2024). **Herding behaviour** is another factor, when people see others sharing information, they're more likely to share it themselves, discounting their own judgment in favour of following the crowd. And **anxiety**, which is often triggered by information overload itself, leads people to share content impulsively as a way of coping with emotional distress.

Problematic social media use amplifies these effects. Users who show signs of excessive or maladaptive social media use are significantly more likely to believe false news and intend to engage with it through clicks, likes and shares (Meshi & Molina, 2025). This creates a troubling feedback loop: the people most susceptible to misinformation are also the heaviest users of the platforms where misinformation spreads.

Importantly, many misinformation studies are crisis-specific (COVID-19, elections), raising questions about generalisability to everyday information environments. There is also limited cross-cultural work, despite strong evidence that trust in institutions, media systems and platform governance varies substantially across contexts.

Social media companies bear significant responsibility for these dynamics. Their algorithms prioritise engaging content, which often means sensational or emotionally charged material, regardless of accuracy (Denniss & Lindberg, 2025). The limited commitment to content moderation, combined with financial incentives that reward engagement over accuracy, means that misinformation continues to thrive despite growing awareness of the problem.

### What Can We Do About It?

The picture painted by this research might seem bleak, but there are evidence-based strategies for combating information overload and improving our decision-making.

#### Individual Strategies

At the individual level, the research points to several helpful approaches. First, **developing information literacy** is crucial. This means learning to critically evaluate sources, recognise emotional manipulation and resist the urge to share content impulsively (Shahrzadi et al., 2024). Second, **chunking and organising information**, breaking complex problems into smaller, manageable pieces, helps work around the limits of working memory (Miller, 1956). Third, being aware of our own biases and deliberately slowing down decision-making can counteract the tendency to rely too heavily on heuristics.

Research on AI-assisted decision-making offers some additional insights. When AI tools are well-designed and transparent about their limitations, humans can sometimes achieve **complementary performance**: outcomes better than either humans or AI could achieve alone (Steyvers & Kumar, 2024). But this requires building appropriate mental models of what AI can and can't do, which is a skill that takes time to develop.

#### Structural and Design Interventions

Beyond individual efforts, there's growing evidence that structural and design interventions can help. In online environments, tools for filtering, categorising and prioritising information can reduce the feeling of overwhelm (Arnold et al., 2023). Organisations can implement policies that reduce unnecessary information load on employees, such as limiting email volume or creating designated "deep work" time.

For misinformation specifically, **nudging interventions** have shown promise. Simple prompts that ask users to consider the accuracy of content before sharing can significantly reduce the spread of false information (Denniss & Lindberg, 2025). Warning labels on potentially misleading content can also help, though their effectiveness depends on design and context.

#### The Role of Regulation

Finally, there's growing recognition that individual-level solutions aren't enough. The mechanisms that drive information overload and misinformation are baked into the design of digital platforms, which means systemic change is needed. Researchers have called for increased monitoring and regulation of social media platforms, including requirements for greater transparency about algorithms and more robust content moderation (Denniss & Lindberg, 2025; Clemons et al., 2025). Some have even suggested that international treaty frameworks, similar to those used for tobacco control, might be necessary to address what amounts to a global public health crisis. We need digital filters to become standard, just as physical filters became standard for cigarettes.

### Looking Ahead

Information overload isn't going away. If anything, the proliferation of AI-generated content promises to accelerate the problem further, creating ever more text, images and video competing for our limited attention. But understanding the mechanisms of overload, how our cognitive limitations interact with the design of digital environments, gives us tools to fight back.

The research is clear: we're not built to handle the information environment we've created. Our working memory has limits. Our decision-making relies on shortcuts that can be exploited. And social dynamics amplify our individual vulnerabilities. But the same research also offers hope. By designing better systems, developing better skills and advocating for better policies, we can create an information landscape that supports rather than undermines good decision-making.

If information overload is a design problem as much as a cognitive one, then improving decision-making may depend less on fixing human limitations and more on taking responsibility for the systems we continue to build around them.

### Emerging research directions

This literature points to several unresolved questions that require deeper investigation:
1. How durable are misinformation interventions?
2. Do accuracy nudges and warning labels produce sustained changes in sharing behaviour over time, or do their effects decay as users habituate?
3. What cognitive mechanisms mediate overload and misinformation?
4. Can better information design meaningfully reduce overload at scale?
5. How do different forms of filtering, ranking and summarisation affect decision quality in real-world digital systems?

Addressing these questions likely requires mixed-method approaches combining experiments, longitudinal surveys and platform-level data. Ethical access to data, transparency of algorithms and reproducibility will be critical challenges, but also central contributions, for future research in this space.

---

**AI assistance disclosure**: While the analysis and perspectives here are mine, I did use generative AI tools to develop this post. I used Google's Notebook LM to help with coding the papers I'd collated. I used Anthropic's Claude Opus 4.5 to fact check my interpretations and format the references. All the sources below are real, non-hallucinated, publicly available papers. 

---

### References

Arnold, M., Goldschmitt, M., & Rigotti, T. (2023). Dealing with information overload: A comprehensive review. *Frontiers in Psychology*, *14*, 1122200. [https://doi.org/10.3389/fpsyg.2023.1122200](https://doi.org/10.3389/fpsyg.2023.1122200)

Boby, J. (2024). An analysis of the impact of choice overload on inducing decision paralysis in the online food ordering industry. *International Journal of Electronic Commerce Studies*, *15*(1), 1-24. [https://doi.org/10.14445/23939125/IJEMS-V11I6P101](https://doi.org/10.14445/23939125/IJEMS-V11I6P101)

Clemons, E. K., Dewan, R. M., Kauffman, R. J., & Weber, T. A. (2025). Managing disinformation on social media platforms. *Electronic Markets*, *35*, 52. [https://doi.org/10.1007/s12525-025-00796-6](https://doi.org/10.1007/s12525-025-00796-6)

Denniss, E., & Lindberg, R. (2025). Social media and the spread of misinformation: Infectious and a threat to public health. *Health Promotion International*, *40*(2), daaf023. [https://doi.org/10.1093/heapro/daaf023](https://doi.org/10.1093/heapro/daaf023)

Eppler, M. J., & Mengis, J. (2004). The concept of information overload: A review of literature from organization science, accounting, marketing, MIS, and related disciplines. *The Information Society*, *20*(5), 325-344. [https://doi.org/10.1080/01972240490507974](https://doi.org/10.1080/01972240490507974)

Huang, Q., Lei, S., & Ni, B. (2022). Perceived information overload and unverified information sharing on WeChat amid the COVID-19 pandemic: A moderated mediation model of anxiety and perceived herd. *Frontiers in Psychology*, *13*, 837820. [https://doi.org/10.3389/fpsyg.2022.837820](https://doi.org/10.3389/fpsyg.2022.837820)

Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of decision under risk. *Econometrica*, *47*(2), 263-291.

Meshi, D., & Molina, M. D. (2025). Problematic social media use is associated with believing in and engaging with fake news. *PLOS ONE*, *20*(5), e0321361. [https://doi.org/10.1371/journal.pone.0321361](https://doi.org/10.1371/journal.pone.0321361)

Miller, G. A. (1956). The magical number seven, plus or minus two: Some limits on our capacity for processing information. *Psychological Review*, *63*(2), 81-97. [https://doi.org/10.1037/h0043158](https://doi.org/10.1037/h0043158)

Misuraca, R., Ferrara, A., & Ferrara, E. (2024). On the advantages and disadvantages of choice: Future research directions in choice overload and its effects on decision-making. *Frontiers in Psychology*, *15*, 1323231. [https://doi.org/10.3389/fpsyg.2024.1290359](https://doi.org/10.3389/fpsyg.2024.1290359)

Muchnik, L., Aral, S., & Taylor, S. J. (2013). Social influence bias: A randomized experiment. *Science*, *341*(6146), 647-651. [https://doi.org/10.1126/science.1240466](https://doi.org/10.1126/science.1240466)

Nguyen, T. V., Ryan, R. M., & Deci, E. L. (2025). Feeds, feelings, and focus: A systematic review and meta-analysis examining the cognitive and mental health correlates of short-form video use. *Journal of Behavioral Addictions*, *14*(1), 1-23. [https://doi.org/10.1037/bul0000498](https://doi.org/10.1037/bul0000498)

Schemmer, M., Heinz, D., Baier, L., Vössing, M., & Kühl, N. (2021). Conceptualizing digital resilience for AI-based information systems. In *Proceedings of the 29th European Conference on Information Systems (ECIS)*. [https://aisel.aisnet.org/ecis2021_rip/44](https://aisel.aisnet.org/ecis2021_rip/44)

Shahrzadi, L., Mansouri, A., & Nikakhlag, S. (2024). Causes, consequences, and strategies to deal with information overload: A scoping review. *International Journal of Information Management Data Insights*, *4*, 100261. [https://doi.org/10.1016/j.jjimei.2024.100261](https://doi.org/10.1016/j.jjimei.2024.100261)

Simon, H. A. (1955). A behavioral model of rational choice. *The Quarterly Journal of Economics*, *69*(1), 99-118. [https://doi.org/10.2307/1884852](https://doi.org/10.2307/1884852)

Steyvers, M., & Kumar, A. (2024). Three challenges for AI-assisted decision-making. *Perspectives on Psychological Science*, *19*(4), 722-734. [https://doi.org/10.1177/17456916231181102](https://doi.org/10.1177/17456916231181102)

Thomas, A. G., Finkel, E. J., & Eastwick, P. W. (2025). Decision-making on dating apps: Is swiping more, less, and swiping right wrong? *Media Psychology*. [https://doi.org/10.1080/15213269.2025.2555430](https://doi.org/10.1080/15213269.2025.2555430)

Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases. *Science*, *185*(4157), 1124-1131. [https://www.jstor.org/stable/1738360](https://www.jstor.org/stable/1738360)

Yu, L., Chen, X., & Wang, Y. (2024). Navigating the digital landscape: Challenges and barriers to effective information use on the internet. *Encyclopedia*, *4*(4), 1665-1680. [https://doi.org/10.3390/encyclopedia4040109](https://doi.org/10.3390/encyclopedia4040109)

Zhang, Z., Cheng, Z., Gu, T., & Zhang, Y. (2024). Determinants of users' unverified information sharing on social media platforms: A herding behavior perspective. *Acta Psychologica*, *248*, 104345. [https://doi.org/10.1016/j.actpsy.2024.104345](https://doi.org/10.1016/j.actpsy.2024.104345)