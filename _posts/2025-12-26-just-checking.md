---
layout: post
title: "Just Checking: Information Seeking Behaviour in the Age of Generative AI"
date: 2025-12-26
description: "Literature review of trust, verification, and governance in information seeking with generative AI."
permalink: /2025-12-26-just-checking
tags: [generative-ai, information-seeking, trust, verification, knowledge-governance]
categories: [research, information-management]
author: "Lewis Dryburgh"
---

### Just Checking: Information Seeking Behaviour in the Age of Generative AI

#### Introduction

The emergence of generative artificial intelligence (GenAI) has fundamentally transformed how individuals seek, evaluate and consume information. Large language models (LLMs) such as ChatGPT and Microsoft Copilot have introduced conversational search paradigms that blur the traditional boundaries between query-based information retrieval and interactive dialogue (Mayerhofer et al., 2025). This literature review examines the evolving landscape of information seeking behaviour in the context of GenAI. I review recent empirical work on three linked topics: the psychological foundations of information seeking, trust in AI-mediated environments and implications for information credibility and integrity.

#### Psychological Foundations of Information Seeking

Understanding how and why individuals seek information requires grounding in established psychological frameworks. Information seeking behaviour has been conceptualised through multiple theoretical lenses, including sense-making (Dervin, 1983), berrypicking (Bates, 1989) and the contextual model of information behaviour (Wilson, 1999). These perspectives emphasise the dynamic, iterative nature of information seeking and show how cognitive and emotional states shape search strategies. Recent scholarship has sought to update these frameworks for contemporary digital environments (Wilson, 2024), emphasising the importance of contextual factors and noting that information seeking habits are not merely individual cognitive processes but are embedded within social, technological and situational contexts (Savolainen, 2024).

Research on the psychological drivers of information seeking has revealed that emotional states significantly influence search behaviour. Anxious individuals demonstrate increased information-seeking tendencies, particularly in response to large environmental changes or perceived threats (Charpentier et al., 2022). This heightened vigilance may serve adaptive functions in uncertain environments but can also lead to maladaptive patterns such as excessive reassurance-seeking or rumination. The COVID-19 pandemic provided a naturalistic context for observing these dynamics at scale, as individuals navigated unprecedented informational uncertainty regarding health, policy and social norms.

AI-powered search engines complicate these processes. Empirical studies examining user behaviour with AI-powered engines have documented distinctive interaction patterns compared to traditional search (Pham et al., 2024). Users engage in longer, more conversational queries and demonstrate different evaluation strategies when processing AI-generated responses. These behavioural shifts suggest that GenAI elicits distinct information-gathering strategies that mirror social conversation more than archival retrieval, though the extent to which these represent fundamental cognitive changes remains an open empirical question.

These psychological dynamics have direct implications for how users evaluate AI-generated information. Heightened vigilance among anxious seekers can increase susceptibility to misleading credibility cues: motivated to resolve uncertainty quickly, users may accept formal markers of authority—such as citations or a confident tone—without deeper verification. Similarly, the conversational engagement that characterises AI interactions may reduce the critical distance users typically maintain when evaluating sources, creating conditions where affective rapport competes with epistemic scrutiny. These connections between emotional states and credibility assessment form a critical bridge to understanding trust dynamics in AI-mediated environments.

#### Trust in AI-Generated Information

Trust represents a critical mediator between information exposure and its influence on beliefs and behaviours (Huynh & Aichner, 2025). A substantial body of recent research has examined trust dynamics specific to GenAI systems, revealing nuanced and sometimes paradoxical findings that can be organised around two key themes: the influence of interface design cues and the role of user characteristics.

#### The Influence of Interface Design Cues

Large-scale experimental research has established that specific design features significantly modulate trust in GenAI outputs (Li & Aral, 2025). Alarmingly, reference links and citations increase trust even when they are incorrect or fabricated (Li & Aral, 2025). This vulnerability intersects with findings on anthropomorphism: users who perceive AI systems as more human-like also demonstrate higher trust and greater willingness to trade factual accuracy for enhanced personalisation and conversational flow (Yazan et al., 2025; Huschens et al., 2023). Together, these findings show that both formal credibility markers and relational qualities can override epistemic caution.

Transparency can backfire: signalling uncertainty often reduces trust, and may discourage users from relying on correct information (Li & Aral, 2025). This tension between epistemic responsibility and user acceptance presents significant design challenges for AI systems intended to support informed decision-making.

#### The Role of User Characteristics

While design choices shape the information environment, trust responses also vary systematically across user populations. Demographic factors including age, education and prior AI experience correlate with differential trust patterns (Yazan et al., 2025). Notably, middle-aged adults demonstrate a potentially vulnerable configuration: higher expressed trust in ChatGPT combined with lower frequency of use, suggesting less opportunity to develop calibrated expectations through experience. In contrast, users who employ both ChatGPT and traditional search daily exhibit higher trust and greater anthropomorphization—but also presumably more opportunities to encounter and potentially recognise system limitations.

Design interventions should be tailored to user groups: adapt presentation to users' prior expectations and provide opportunities to build calibrated expectations.

#### Health Information and Credibility Perceptions

The domain of health information seeking provides an especially consequential context for examining AI trust dynamics, given the potential for misinformation to influence medical decision-making. Mixed-methods research combining surveys and laboratory studies with physiological sensing has revealed that LLM-generated health information is trusted more than human-generated content when source provenance is unknown (Sun et al., 2025). However, when information is explicitly labelled by source, human-attributed content receives higher trust ratings than AI-attributed content (Sun et al., 2025).

The mismatch between actual and perceived source shows that transparency and labelling strongly shape credibility judgements (Sun et al., 2024). From a design perspective, this creates both opportunities and risks: adding transparency labels indicating AI generation reduces unwarranted trust, but such labelling requires institutional commitment and user awareness that may not be consistently present across information environments.

Advanced sensing approaches incorporating eye-tracking and physiological measures (ECG, EDA, skin temperature) have demonstrated that trust responses manifest in measurable behavioural and physiological signatures (Sun et al., 2025; Ji et al., 2024). Machine learning models trained on these features achieved 73% accuracy in predicting self-reported trust levels and 65% accuracy in identifying whether content was AI or human-generated based on user responses alone (Sun et al., 2025). These findings suggest possibilities for real-time trust verification and the potential development of interfaces responsive to user credibility assessments.

#### Implications for Information Ecosystem Integrity

The integration of generative AI into information ecosystems raises systemic concerns extending beyond individual trust calibration (Hirvonen et al., 2024). GenAI systems increasingly serve as intermediaries in information curation, affecting exposure to diverse perspectives and potentially amplifying or mitigating misinformation (Jarrahi et al., 2025). From the seeker's perspective, this creates an environment where the provenance and reliability of information are increasingly difficult to assess independently.

Comparative analyses of LLM-based chatbots' abilities to verify factual claims have shown that ChatGPT correctly evaluated approximately 72% of true, false and borderline political statements, with significant variability across languages and topics (Kuznetsova et al., 2025). High-resource languages such as English demonstrated better performance than low-resource languages, raising equity concerns about differential misinformation vulnerability across linguistic communities. For individual seekers, these accuracy limitations mean that verification strategies developed for traditional sources—such as cross-referencing multiple outlets—may be insufficient when multiple AI systems draw from overlapping training data.

The challenge of user verification is compounded by system-level design choices. The concept of confidence-based response abstinence—where AI systems decline to answer when uncertainty is high—represents one proposed approach to managing epistemic risk (Huang et al., 2024). However, this strategy requires accurate self-assessment of model capabilities and may conflict with user expectations for responsive assistance. Research on tactics users employ when navigating between traditional and conversational search has revealed that verification behaviours often involve toggling between modalities, using traditional search to fact-check AI outputs or AI to synthesise results from conventional queries (Mayerhofer et al., 2025). Privacy and security concerns additionally shape willingness to engage in these verification processes, with systematic reviews identifying key perceptual factors that influence adoption and continued use of conversational systems (Leschanowsky et al., 2024).

#### Synthesis and Future Directions

The literature reviewed here converges on several key insights. First, trust in AI-generated information is neither uniformly high nor low but is actively shaped by design choices, contextual factors and user characteristics (Li & Aral, 2025; Huynh & Aichner, 2025). Second, the markers of credibility that users rely upon—citations, references, formal presentation—can be manipulated by generative systems, creating new vectors for misinformation (Li & Aral, 2025). Third, emotional and relational factors including anxiety, anthropomorphism and conversational engagement influence information seeking and evaluation in ways that may privilege experiential qualities over epistemic accuracy (Yazan et al., 2025; Charpentier et al., 2022).

Addressing GenAI's risks calls for coordinated action: improve interfaces to encourage proper trust, require platform transparency and expand AI literacy education. The interdisciplinary nature of these challenges necessitates continued collaboration across information science, human-computer interaction, cognitive psychology and policy studies.

Future research should extend current findings longitudinally to assess how trust calibration evolves with experience, examine cross-cultural variation in AI credibility perceptions and develop and evaluate interventions designed to promote healthy information seeking practices in AI-mediated environments. As generative AI enters everyday use, researchers and policymakers must study and shape its effects on how people find and use knowledge.

### References

Bates, M. J. (1989). The design of browsing and berrypicking techniques for the online search interface. *Online Review, 13*(5), 407–424. https://doi.org/10.1108/eb024320

Charpentier, C. J., Cogliati Dezza, I., Vellani, V., Globig, L. K., Gädeke, M., & Sharot, T. (2022). Anxiety increases information-seeking in response to large changes. *Scientific Reports, 12*, 7385. https://doi.org/10.1038/s41598-022-10813-9

Dervin, B. (1983). An overview of sense-making research: Concepts, methods and results. Paper presented at the Annual Meeting of the International Communication Association, Dallas, TX.

Hirvonen, N., Jylhä, V., Lao, Y., & Larsson, S. (2024). Artificial intelligence in the information ecosystem: Affordances for everyday information seeking. *Journal of the Association for Information Science and Technology, 75*(10), 1152–1165. https://doi.org/10.1002/asi.24860

Huang, Y., et al. (2024). Confidence-based response abstinence: Improving LLM trustworthiness via activation-based uncertainty estimation. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing*. ACL. https://aclanthology.org/2024.emnlp-main.1040/

Huschens, M., et al. (2023). Do you trust ChatGPT? Perceived credibility of human and AI-generated content. *arXiv preprint arXiv:2309.02524*. https://arxiv.org/abs/2309.02524

Huynh, M.-T., & Aichner, T. (2025). In generative artificial intelligence we trust: Unpacking determinants and outcomes for cognitive trust. *AI & Society, 40*, 5849–5869. https://doi.org/10.1007/s00146-025-02378-8

Jarrahi, M. H., Li, L., Robinson, A. P., & Meng, S. (2025). Generative AI and the augmentation of information practices in knowledge work. *Behaviour & Information Technology*. Advance online publication. https://doi.org/10.1080/0144929X.2025.2551570

Ji, K., Hettiachchi, D., Salim, F. D., Scholer, F., & Spina, D. (2024). Characterizing information seeking processes with multiple physiological signals. In *Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24)*. ACM. https://doi.org/10.1145/3626772.3657793

Kuznetsova, E., Makhortykh, M., Vziatysheva, V., Stolze, M., Baghumyan, A., & Urman, A. (2025). In generative AI we trust: Can chatbots effectively verify political information? *Journal of Computational Social Science, 8*, 15. https://doi.org/10.1007/s42001-024-00338-8

Leschanowsky, A., Rech, S., Popp, B., & Bäckström, T. (2024). Evaluating privacy, security, and trust perceptions in conversational AI: A systematic review. *Computers in Human Behavior, 161*, 108344. https://doi.org/10.1016/j.chb.2024.108344

Li, H., & Aral, S. (2025). Human trust in AI search: A large-scale experiment. *arXiv preprint arXiv:2504.06435*. https://arxiv.org/abs/2504.06435

Mayerhofer, A., et al. (2025). Blending queries and conversations: Understanding tactics, trust, verification, and system choice in web search and chat interactions. *arXiv preprint arXiv:2504.07979*. https://arxiv.org/abs/2504.07979

Pham, V. K., Pham Thi, T. D., & Duong, N. T. (2024). A study on information search behavior using AI-powered engines: Evidence from chatbots on online shopping platforms. *SAGE Open, 14*(4). https://doi.org/10.1177/21582440241298706

Savolainen, R. (2024). Approaching information-seeking habits and their contextual features. *Information Research, 29*(2). https://informationr.net/ir/29-2/paper978.html

Sun, X., Ma, R., Wei, S., Cesar, P., Bosch, J. A., & El Ali, A. (2025). Understanding trust toward human versus AI-generated health information through behavioral and physiological sensing. *International Journal of Human-Computer Studies*. Advance online publication. https://arxiv.org/abs/2512.12348

Sun, X., Ma, R., Zhao, X., Li, Z., Lindqvist, J., El Ali, A., & Bosch, J. A. (2024). Trusting the search: Unraveling human trust in health information from Google and ChatGPT. *arXiv preprint arXiv:2403.09987*. https://doi.org/10.48550/arXiv.2403.09987

Wilson, T. D. (1999). Models in information behaviour research. *Journal of Documentation, 55*(3), 249–270. https://doi.org/10.1108/EUM0000000007145

Wilson, T. D. (2024). Approaches to information-seeking behaviour in psychology. *Information Research, 29*(1). https://informationr.net/ir/29-1/paper966.html

Yazan, M., Situmeang, F. B. I., & Verberne, S. (2025). Personality over precision: Exploring the influence of human-likeness on ChatGPT use for search. In *Proceedings of NIP@IR 2025*. https://doi.org/10.48550/arXiv.2511.06447
---
layout: post
title: "Just Checking: Information Seeking Behaviour in the Age of Generative AI"
date: 2025-12-26
description: "Literature review of trust, verification, and governance in information seeking with generative AI."
permalink: /2025-12-26-just-checking
tags: [generative-ai, information-seeking, trust, verification, knowledge-governance]
categories: [research, information-management]
author: "Lewis Dryburgh"
---

### Just Checking: Information Seeking Behaviour in the Age of Generative AI

#### Introduction

The emergence of generative artificial intelligence (GenAI) has fundamentally transformed how individuals seek, evaluate and consume information. Large language models (LLMs) such as ChatGPT and Microsoft Copilot have introduced conversational search paradigms that blur the traditional boundaries between query-based information retrieval and interactive dialogue (Mayerhofer et al., 2025). This literature review examines the evolving landscape of information seeking behavior in the context of GenAI, synthesizing recent empirical research across three interconnected domains: the psychological foundations of information seeking, trust dynamics in AI-mediated information environments and the implications for information credibility and integrity.

#### Psychological Foundations of Information Seeking

Understanding how and why individuals seek information requires grounding in established psychological frameworks. Information seeking behaviour has been conceptualised through multiple theoretical lenses, including sense-making (Dervin, 1983), berrypicking (Bates, 1989) and the contextual model of information behaviour (Wilson, 1999). These perspectives emphasise the dynamic, iterative nature of information seeking and show how cognitive and emotional states shape search strategies. Recent scholarship has sought to update these frameworks for contemporary digital environments (Wilson, 2024), emphasising the importance of contextual factors and noting that information seeking habits are not merely individual cognitive processes but are embedded within social, technological and situational contexts (Savolainen, 2024).

Research on the psychological drivers of information seeking has revealed that emotional states significantly influence search behaviour. Anxious individuals demonstrate increased information-seeking tendencies, particularly in response to large environmental changes or perceived threats (Charpentier et al., 2022). This heightened vigilance may serve adaptive functions in uncertain environments but can also lead to maladaptive patterns such as excessive reassurance-seeking or rumination. The COVID-19 pandemic provided a naturalistic context for observing these dynamics at scale, as individuals navigated unprecedented informational uncertainty regarding health, policy and social norms.

AI-powered search engines complicate these processes. Empirical studies examining user behaviour with AI-powered engines have documented distinctive interaction patterns compared to traditional search (Pham et al., 2024). Users engage in longer, more conversational queries and demonstrate different evaluation strategies when processing AI-generated responses. These behavioural shifts suggest that GenAI elicits distinct information-gathering strategies that mirror social conversation more than archival retrieval, though the extent to which these represent fundamental cognitive changes remains an open empirical question.

These psychological dynamics have direct implications for how users evaluate AI-generated information. Heightened vigilance among anxious seekers can increase susceptibility to misleading credibility cues: motivated to resolve uncertainty quickly, users may accept formal markers of authority—such as citations or a confident tone—without deeper verification. Similarly, the conversational engagement that characterises AI interactions may reduce the critical distance users typically maintain when evaluating sources, creating conditions where affective rapport competes with epistemic scrutiny. These connections between emotional states and credibility assessment form a critical bridge to understanding trust dynamics in AI-mediated environments.

#### Trust in AI-Generated Information

Trust represents a critical mediator between information exposure and its influence on beliefs and behaviors (Huynh & Aichner, 2025). A substantial body of recent research has examined trust dynamics specific to GenAI systems, revealing nuanced and sometimes paradoxical findings that can be organized around two key themes: the influence of interface design cues and the role of user characteristics.

#### The Influence of Interface Design Cues

Large-scale experimental research has established that specific design features significantly modulate trust in GenAI outputs (Li & Aral, 2025). Most concerning from an epistemic perspective, reference links and citations substantially increase trust, even when those references are incorrect or entirely fabricated (Li & Aral, 2025). This vulnerability intersects with findings on anthropomorphism: users who perceive AI systems as more human-like also demonstrate higher trust and greater willingness to trade factual accuracy for enhanced personalization and conversational flow (Yazan et al., 2025; Huschens et al., 2023). Together, these findings suggest that both formal credibility markers and relational qualities can override epistemic caution.

Conversely, transparency features produce counterintuitive effects. Uncertainty highlighting—where GenAI explicitly communicates its confidence levels—reduces user trust regardless of whether confidence is high or low, suggesting that transparency about limitations may paradoxically discourage user reliance even when epistemically appropriate (Li & Aral, 2025). This tension between epistemic responsibility and user acceptance presents significant design challenges for AI systems intended to support informed decision-making.

#### The Role of User Characteristics

While design choices shape the information environment, trust responses also vary systematically across user populations. Demographic factors including age, education and prior AI experience correlate with differential trust patterns (Yazan et al., 2025). Notably, middle-aged adults demonstrate a potentially vulnerable configuration: higher expressed trust in ChatGPT combined with lower frequency of use, suggesting less opportunity to develop calibrated expectations through experience. In contrast, users who employ both ChatGPT and traditional search daily exhibit higher trust and greater anthropomorphization—but also presumably more opportunities to encounter and potentially recognize system limitations.

Design interventions should be tailored to user groups: adapt presentation to users' prior expectations and provide opportunities to build calibrated expectations.

#### Health Information and Credibility Perceptions

The domain of health information seeking provides an especially consequential context for examining AI trust dynamics, given the potential for misinformation to influence medical decision-making. Mixed-methods research combining surveys and laboratory studies with physiological sensing has revealed that LLM-generated health information is trusted more than human-generated content when source provenance is unknown (Sun et al., 2025). However, when information is explicitly labeled by source, human-attributed content receives higher trust ratings than AI-attributed content (Sun et al., 2025).

This dissociation between actual source and perceived source suggests that credibility perceptions are significantly influenced by transparency practices and labeling conventions (Sun et al., 2024). From a design perspective, this creates both opportunities and risks: adding transparency labels indicating AI generation reduces unwarranted trust, but such labeling requires institutional commitment and user awareness that may not be consistently present across information environments.

Advanced sensing approaches incorporating eye-tracking and physiological measures (ECG, EDA, skin temperature) have demonstrated that trust responses manifest in measurable behavioral and physiological signatures (Sun et al., 2025; Ji et al., 2024). Machine learning models trained on these features achieved 73% accuracy in predicting self-reported trust levels and 65% accuracy in identifying whether content was AI or human-generated based on user responses alone (Sun et al., 2025). These findings suggest possibilities for real-time trust verification and the potential development of interfaces responsive to user credibility assessments.

#### Implications for Information Ecosystem Integrity

The integration of generative AI into information ecosystems raises systemic concerns extending beyond individual trust calibration (Hirvonen et al., 2024). GenAI systems increasingly serve as intermediaries in information curation, affecting exposure to diverse perspectives and potentially amplifying or mitigating misinformation (Jarrahi et al., 2025). From the seeker's perspective, this creates an environment where the provenance and reliability of information are increasingly difficult to assess independently.

Comparative analyses of LLM-based chatbots' abilities to verify factual claims have shown that ChatGPT correctly evaluated approximately 72% of true, false and borderline political statements, with significant variability across languages and topics (Kuznetsova et al., 2025). High-resource languages such as English demonstrated better performance than low-resource languages, raising equity concerns about differential misinformation vulnerability across linguistic communities. For individual seekers, these accuracy limitations mean that verification strategies developed for traditional sources—such as cross-referencing multiple outlets—may be insufficient when multiple AI systems draw from overlapping training data.

The challenge of user verification is compounded by system-level design choices. The concept of confidence-based response abstinence—where AI systems decline to answer when uncertainty is high—represents one proposed approach to managing epistemic risk (Huang et al., 2024). However, this strategy requires accurate self-assessment of model capabilities and may conflict with user expectations for responsive assistance. Research on tactics users employ when navigating between traditional and conversational search has revealed that verification behaviors often involve toggling between modalities, using traditional search to fact-check AI outputs or AI to synthesize results from conventional queries (Mayerhofer et al., 2025). Privacy and security concerns additionally shape willingness to engage in these verification processes, with systematic reviews identifying key perceptual factors that influence adoption and continued use of conversational systems (Leschanowsky et al., 2024).

#### Synthesis and Future Directions

The literature reviewed here converges on several key insights. First, trust in AI-generated information is neither uniformly high nor low but is actively shaped by design choices, contextual factors and user characteristics (Li & Aral, 2025; Huynh & Aichner, 2025). Second, the markers of credibility that users rely upon—citations, references, formal presentation—can be manipulated by generative systems, creating new vectors for misinformation (Li & Aral, 2025). Third, emotional and relational factors including anxiety, anthropomorphism and conversational engagement influence information seeking and evaluation in ways that may privilege experiential qualities over epistemic accuracy (Yazan et al., 2025; Charpentier et al., 2022).

These findings suggest that addressing the challenges of GenAI in information environments requires interventions across multiple levels: interface design choices that promote appropriate trust calibration, platform policies regarding transparency and labeling and user education initiatives building AI literacy. The interdisciplinary nature of these challenges necessitates continued collaboration across information science, human-computer interaction, cognitive psychology and policy studies.

Future research should extend current findings longitudinally to assess how trust calibration evolves with experience, examine cross-cultural variation in AI credibility perceptions and develop and evaluate interventions designed to promote healthy information seeking practices in AI-mediated environments. As generative AI becomes increasingly embedded in everyday information seeking, understanding and shaping its effects on human knowledge practices becomes not merely an academic concern but a societal imperative.

### References

Bates, M. J. (1989). The design of browsing and berrypicking techniques for the online search interface. *Online Review, 13*(5), 407–424. https://doi.org/10.1108/eb024320

Charpentier, C. J., Cogliati Dezza, I., Vellani, V., Globig, L. K., Gädeke, M., & Sharot, T. (2022). Anxiety increases information-seeking in response to large changes. *Scientific Reports, 12*, 7385. https://doi.org/10.1038/s41598-022-10813-9

Dervin, B. (1983). An overview of sense-making research: Concepts, methods and results. Paper presented at the Annual Meeting of the International Communication Association, Dallas, TX.

Hirvonen, N., Jylhä, V., Lao, Y., & Larsson, S. (2024). Artificial intelligence in the information ecosystem: Affordances for everyday information seeking. *Journal of the Association for Information Science and Technology, 75*(10), 1152–1165. https://doi.org/10.1002/asi.24860

Huang, Y., et al. (2024). Confidence-based response abstinence: Improving LLM trustworthiness via activation-based uncertainty estimation. In *Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing*. ACL. https://aclanthology.org/2024.emnlp-main.1040/

Huschens, M., et al. (2023). Do you trust ChatGPT? Perceived credibility of human and AI-generated content. *arXiv preprint arXiv:2309.02524*. https://arxiv.org/abs/2309.02524

Huynh, M.-T., & Aichner, T. (2025). In generative artificial intelligence we trust: Unpacking determinants and outcomes for cognitive trust. *AI & Society, 40*, 5849–5869. https://doi.org/10.1007/s00146-025-02378-8

Jarrahi, M. H., Li, L., Robinson, A. P., & Meng, S. (2025). Generative AI and the augmentation of information practices in knowledge work. *Behaviour & Information Technology*. Advance online publication. https://doi.org/10.1080/0144929X.2025.2551570

Ji, K., Hettiachchi, D., Salim, F. D., Scholer, F., & Spina, D. (2024). Characterizing information seeking processes with multiple physiological signals. In *Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '24)*. ACM. https://doi.org/10.1145/3626772.3657793

Kuznetsova, E., Makhortykh, M., Vziatysheva, V., Stolze, M., Baghumyan, A., & Urman, A. (2025). In generative AI we trust: Can chatbots effectively verify political information? *Journal of Computational Social Science, 8*, 15. https://doi.org/10.1007/s42001-024-00338-8

Leschanowsky, A., Rech, S., Popp, B., & Bäckström, T. (2024). Evaluating privacy, security, and trust perceptions in conversational AI: A systematic review. *Computers in Human Behavior, 161*, 108344. https://doi.org/10.1016/j.chb.2024.108344

Li, H., & Aral, S. (2025). Human trust in AI search: A large-scale experiment. *arXiv preprint arXiv:2504.06435*. https://arxiv.org/abs/2504.06435

Mayerhofer, A., et al. (2025). Blending queries and conversations: Understanding tactics, trust, verification, and system choice in web search and chat interactions. *arXiv preprint arXiv:2504.07979*. https://arxiv.org/abs/2504.07979

Pham, V. K., Pham Thi, T. D., & Duong, N. T. (2024). A study on information search behavior using AI-powered engines: Evidence from chatbots on online shopping platforms. *SAGE Open, 14*(4). https://doi.org/10.1177/21582440241298706

Savolainen, R. (2024). Approaching information-seeking habits and their contextual features. *Information Research, 29*(2). https://informationr.net/ir/29-2/paper978.html

Sun, X., Ma, R., Wei, S., Cesar, P., Bosch, J. A., & El Ali, A. (2025). Understanding trust toward human versus AI-generated health information through behavioral and physiological sensing. *International Journal of Human-Computer Studies*. Advance online publication. https://arxiv.org/abs/2512.12348

Sun, X., Ma, R., Zhao, X., Li, Z., Lindqvist, J., El Ali, A., & Bosch, J. A. (2024). Trusting the search: Unraveling human trust in health information from Google and ChatGPT. *arXiv preprint arXiv:2403.09987*. https://doi.org/10.48550/arXiv.2403.09987

Wilson, T. D. (1999). Models in information behaviour research. *Journal of Documentation, 55*(3), 249–270. https://doi.org/10.1108/EUM0000000007145

Wilson, T. D. (2024). Approaches to information-seeking behaviour in psychology. *Information Research, 29*(1). https://informationr.net/ir/29-1/paper966.html

Yazan, M., Situmeang, F. B. I., & Verberne, S. (2025). Personality over precision: Exploring the influence of human-likeness on ChatGPT use for search. In *Proceedings of NIP@IR 2025*. https://doi.org/10.48550/arXiv.2511.06447